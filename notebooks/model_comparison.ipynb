{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa77f9f",
   "metadata": {},
   "source": [
    "# Steam Games Success Prediction - Model Comparison\n",
    "\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "\n",
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b09839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import joblib\n",
    "import time\n",
    "from data_preprocessing import base_pipeline, final_cleaning_pipeline, scaling_pipeline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b4f05",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18241b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline.set_params(data_loading__filepath=\"../data/raw/games.csv\")\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "pre_outlier_df = base_pipeline.fit_transform(None)\n",
    "pre_scaling_df = final_cleaning_pipeline.fit_transform(pre_outlier_df)\n",
    "df = scaling_pipeline.fit_transform(pre_scaling_df)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506ee1d",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"estimated_owners_calculated\"\n",
    "y = df[target_column].copy()\n",
    "\n",
    "columns_to_ignore = [\n",
    "    target_column,\n",
    "    \"average_playtime_forever\",\n",
    "    \"median_playtime_forever\",\n",
    "]\n",
    "\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "feature_columns = [col for col in numeric_features if col not in columns_to_ignore]\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "\n",
    "# Stratified Train-Test Split (gleicher Split wie in den einzelnen Notebooks)\n",
    "n_bins = 5\n",
    "y_binned = pd.cut(y, bins=n_bins, labels=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y_binned\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e8c27",
   "metadata": {},
   "source": [
    "## 4. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Random Forest model...\")\n",
    "rf_model = joblib.load(\"../models/random_forest/model.joblib\")\n",
    "rf_features = joblib.load(\"../models/random_forest/feature_columns.joblib\")\n",
    "rf_scaler = joblib.load(\"../models/random_forest/scaling_pipeline.joblib\")\n",
    "print(f\"Random Forest loaded successfully. Features: {len(rf_features)}\")\n",
    "\n",
    "print(\"\\nLoading Gradient Boosting model...\")\n",
    "gb_model = joblib.load(\"../models/gradient_boosting/model.joblib\")\n",
    "gb_features = joblib.load(\"../models/gradient_boosting/feature_columns.joblib\")\n",
    "gb_scaler = joblib.load(\"../models/gradient_boosting/scaling_pipeline.joblib\")\n",
    "print(f\"Gradient Boosting loaded successfully. Features: {len(gb_features)}\")\n",
    "\n",
    "print(\"\\nLoading XGBoost model...\")\n",
    "xgb_model = joblib.load(\"../models/xgb_regressor/model.joblib\")\n",
    "xgb_features = joblib.load(\"../models/xgb_regressor/feature_columns.joblib\")\n",
    "xgb_scaler = joblib.load(\"../models/xgb_regressor/scaling_pipeline.joblib\")\n",
    "print(f\"XGBoost loaded successfully. Features: {len(xgb_features)}\")\n",
    "\n",
    "print(\"\\nAll models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde0196",
   "metadata": {},
   "source": [
    "## 5. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d07ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating predictions...\")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "rf_pred_time = time.time() - start_time\n",
    "print(f\"Random Forest prediction time: {rf_pred_time:.3f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "gb_train_pred = gb_model.predict(X_train)\n",
    "gb_test_pred = gb_model.predict(X_test)\n",
    "gb_pred_time = time.time() - start_time\n",
    "print(f\"Gradient Boosting prediction time: {gb_pred_time:.3f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_time = time.time() - start_time\n",
    "print(f\"XGBoost prediction time: {xgb_pred_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nAll predictions generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576d165",
   "metadata": {},
   "source": [
    "## 6. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return r2, mae, rmse\n",
    "\n",
    "models_metrics = {}\n",
    "\n",
    "rf_train_r2, rf_train_mae, rf_train_rmse = calculate_metrics(y_train, rf_train_pred)\n",
    "rf_test_r2, rf_test_mae, rf_test_rmse = calculate_metrics(y_test, rf_test_pred)\n",
    "models_metrics['Random Forest'] = {\n",
    "    'train_r2': rf_train_r2, 'test_r2': rf_test_r2,\n",
    "    'train_mae': rf_train_mae, 'test_mae': rf_test_mae,\n",
    "    'train_rmse': rf_train_rmse, 'test_rmse': rf_test_rmse,\n",
    "    'pred_time': rf_pred_time\n",
    "}\n",
    "\n",
    "gb_train_r2, gb_train_mae, gb_train_rmse = calculate_metrics(y_train, gb_train_pred)\n",
    "gb_test_r2, gb_test_mae, gb_test_rmse = calculate_metrics(y_test, gb_test_pred)\n",
    "models_metrics['Gradient Boosting'] = {\n",
    "    'train_r2': gb_train_r2, 'test_r2': gb_test_r2,\n",
    "    'train_mae': gb_train_mae, 'test_mae': gb_test_mae,\n",
    "    'train_rmse': gb_train_rmse, 'test_rmse': gb_test_rmse,\n",
    "    'pred_time': gb_pred_time\n",
    "}\n",
    "\n",
    "xgb_train_r2, xgb_train_mae, xgb_train_rmse = calculate_metrics(y_train, xgb_train_pred)\n",
    "xgb_test_r2, xgb_test_mae, xgb_test_rmse = calculate_metrics(y_test, xgb_test_pred)\n",
    "models_metrics['XGBoost'] = {\n",
    "    'train_r2': xgb_train_r2, 'test_r2': xgb_test_r2,\n",
    "    'train_mae': xgb_train_mae, 'test_mae': xgb_test_mae,\n",
    "    'train_rmse': xgb_train_rmse, 'test_rmse': xgb_test_rmse,\n",
    "    'pred_time': xgb_pred_time\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_metrics).T\n",
    "comparison_df['overfitting'] = comparison_df['train_r2'] - comparison_df['test_r2']\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<18} {'Train R²':<10} {'Test R²':<10} {'Test MAE':<10} {'Test RMSE':<10} {'Overfitting':<12} {'Pred Time':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, metrics in models_metrics.items():\n",
    "    overfitting = metrics['train_r2'] - metrics['test_r2']\n",
    "    print(f\"{model_name:<18} {metrics['train_r2']:<10.3f} {metrics['test_r2']:<10.3f} {metrics['test_mae']:<10.0f} {metrics['test_rmse']:<10.0f} {overfitting:<12.3f} {metrics['pred_time']:<12.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best performing model by metric:\")\n",
    "print(f\"Highest Test R²: {comparison_df['test_r2'].idxmax()} ({comparison_df['test_r2'].max():.3f})\")\n",
    "print(f\"Lowest Test MAE: {comparison_df['test_mae'].idxmin()} ({comparison_df['test_mae'].min():.0f})\")\n",
    "print(f\"Lowest Test RMSE: {comparison_df['test_rmse'].idxmin()} ({comparison_df['test_rmse'].min():.0f})\")\n",
    "print(f\"Lowest Overfitting: {comparison_df['overfitting'].idxmin()} ({comparison_df['overfitting'].min():.3f})\")\n",
    "print(f\"Fastest Prediction: {comparison_df['pred_time'].idxmin()} ({comparison_df['pred_time'].min():.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe27b1",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. R² Score Comparison\n",
    "models = list(models_metrics.keys())\n",
    "train_r2_scores = [models_metrics[model]['train_r2'] for model in models]\n",
    "test_r2_scores = [models_metrics[model]['test_r2'] for model in models]\n",
    "\n",
    "fig_r2 = go.Figure()\n",
    "fig_r2.add_trace(go.Bar(\n",
    "    name='Training R²',\n",
    "    x=models,\n",
    "    y=train_r2_scores,\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "fig_r2.add_trace(go.Bar(\n",
    "    name='Test R²',\n",
    "    x=models,\n",
    "    y=test_r2_scores,\n",
    "    marker_color='darkblue'\n",
    "))\n",
    "\n",
    "fig_r2.update_layout(\n",
    "    title='R² Score Comparison - Training vs Test',\n",
    "    xaxis_title='Models',\n",
    "    yaxis_title='R² Score',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=500\n",
    ")\n",
    "fig_r2.show()\n",
    "\n",
    "# 2. Error Metrics Comparison\n",
    "fig_errors = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Mean Absolute Error (MAE)', 'Root Mean Square Error (RMSE)'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "test_mae_scores = [models_metrics[model]['test_mae'] for model in models]\n",
    "test_rmse_scores = [models_metrics[model]['test_rmse'] for model in models]\n",
    "\n",
    "fig_errors.add_trace(\n",
    "    go.Bar(x=models, y=test_mae_scores, name='MAE', marker_color='orange'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_errors.add_trace(\n",
    "    go.Bar(x=models, y=test_rmse_scores, name='RMSE', marker_color='red'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_errors.update_layout(\n",
    "    title='Error Metrics Comparison (Test Set)',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig_errors.show()\n",
    "\n",
    "# 3. Overfitting Analysis\n",
    "overfitting_scores = [models_metrics[model]['train_r2'] - models_metrics[model]['test_r2'] for model in models]\n",
    "\n",
    "fig_overfitting = px.bar(\n",
    "    x=models,\n",
    "    y=overfitting_scores,\n",
    "    title='Overfitting Analysis (Train R² - Test R²)',\n",
    "    labels={'x': 'Models', 'y': 'Overfitting (Train R² - Test R²)'},\n",
    "    template='plotly_white',\n",
    "    color=overfitting_scores,\n",
    "    color_continuous_scale='RdYlBu_r'\n",
    ")\n",
    "fig_overfitting.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", annotation_text=\"No Overfitting\")\n",
    "fig_overfitting.update_layout(height=500, showlegend=False)\n",
    "fig_overfitting.show()\n",
    "\n",
    "# 4. Prediction Time Comparison\n",
    "pred_times = [models_metrics[model]['pred_time'] for model in models]\n",
    "\n",
    "fig_time = px.bar(\n",
    "    x=models,\n",
    "    y=pred_times,\n",
    "    title='Prediction Time Comparison',\n",
    "    labels={'x': 'Models', 'y': 'Prediction Time (seconds)'},\n",
    "    template='plotly_white',\n",
    "    color=pred_times,\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig_time.update_layout(height=500, showlegend=False)\n",
    "fig_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4aa9d",
   "metadata": {},
   "source": [
    "## 8. Predicted vs Actual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pred_actual = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Random Forest', 'Gradient Boosting', 'XGBoost'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "fig_pred_actual.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test, y=rf_test_pred,\n",
    "        mode='markers',\n",
    "        name=f'RF (R²={rf_test_r2:.3f})',\n",
    "        marker=dict(color='blue', size=4, opacity=0.6)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_pred_actual.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test, y=gb_test_pred,\n",
    "        mode='markers',\n",
    "        name=f'GB (R²={gb_test_r2:.3f})',\n",
    "        marker=dict(color='green', size=4, opacity=0.6)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_pred_actual.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test, y=xgb_test_pred,\n",
    "        mode='markers',\n",
    "        name=f'XGB (R²={xgb_test_r2:.3f})',\n",
    "        marker=dict(color='red', size=4, opacity=0.6)\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "min_val = y_test.min()\n",
    "max_val = y_test.max()\n",
    "\n",
    "for col in [1, 2, 3]:\n",
    "    fig_pred_actual.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val], y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            name='Perfect Prediction',\n",
    "            line=dict(color='black', dash='dash'),\n",
    "            showlegend=(col == 1)\n",
    "        ),\n",
    "        row=1, col=col\n",
    "    )\n",
    "\n",
    "fig_pred_actual.update_layout(\n",
    "    title='Predicted vs Actual Values - Model Comparison',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "for col in [1, 2, 3]:\n",
    "    fig_pred_actual.update_xaxes(title_text=\"Actual Values\", row=1, col=col)\n",
    "    fig_pred_actual.update_yaxes(title_text=\"Predicted Values\", row=1, col=col)\n",
    "\n",
    "fig_pred_actual.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f48a9",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_,\n",
    "    'model': 'Random Forest'\n",
    "})\n",
    "\n",
    "gb_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': gb_model.feature_importances_,\n",
    "    'model': 'Gradient Boosting'\n",
    "})\n",
    "\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': xgb_model.feature_importances_,\n",
    "    'model': 'XGBoost'\n",
    "})\n",
    "\n",
    "all_importances = pd.concat([rf_importance, gb_importance, xgb_importance])\n",
    "\n",
    "# Get top 10 features by average importance\n",
    "avg_importance = all_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "top_features = avg_importance.head(10).index.tolist()\n",
    "\n",
    "top_importances = all_importances[all_importances['feature'].isin(top_features)]\n",
    "\n",
    "fig_importance = px.bar(\n",
    "    top_importances,\n",
    "    x='importance',\n",
    "    y='feature',\n",
    "    color='model',\n",
    "    orientation='h',\n",
    "    title='Top 10 Feature Importances - Model Comparison',\n",
    "    labels={'importance': 'Feature Importance', 'feature': 'Features'},\n",
    "    template='plotly_white',\n",
    "    barmode='group',\n",
    "    color_discrete_map={\n",
    "        'Random Forest': 'blue',\n",
    "        'Gradient Boosting': 'green',\n",
    "        'XGBoost': 'red'\n",
    "    }\n",
    ")\n",
    "\n",
    "feature_order = avg_importance.head(10).index[::-1]\n",
    "fig_importance.update_layout(\n",
    "    height=600,\n",
    "    yaxis={'categoryorder': 'array', 'categoryarray': feature_order}\n",
    ")\n",
    "fig_importance.show()\n",
    "\n",
    "print(\"Top 10 Features by Average Importance:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (feature, avg_imp) in enumerate(avg_importance.head(10).items()):\n",
    "    rf_imp = rf_importance[rf_importance['feature'] == feature]['importance'].iloc[0]\n",
    "    gb_imp = gb_importance[gb_importance['feature'] == feature]['importance'].iloc[0]\n",
    "    xgb_imp = xgb_importance[xgb_importance['feature'] == feature]['importance'].iloc[0]\n",
    "    \n",
    "    print(f\"{i+1:2d}. {feature:<25}\")\n",
    "    print(f\"    Average: {avg_imp:.3f}\")\n",
    "    print(f\"    RF: {rf_imp:.3f}, GB: {gb_imp:.3f}, XGB: {xgb_imp:.3f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
